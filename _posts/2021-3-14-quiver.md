---
title: "AI, System: Quiver, A Distributed Cache For Deep Learning"
categories:
  - AI
  - System
---

By Abhishek Vijaya Kumar and Muthian Sivathanu from Microsoft Research India. 

The usual deep learning framework like Pytorch, requests *mini-batches* from the global storage in each iteration. A *mini-batch* is consumed by the GPUs then **discarded immediately**, meaning that a reload is required in the following epoch. Worse, multiple machines may access the same dataset simultaneously(e.g., hyper-parameter tuning), causing the remote storage to become a bottleneck of the whole system.

But we can use the local SSD in each compute node. Quiver, is a distributed SSD cache system to increase data locality and ease IO stress in such workload.

## I/O Characteristics of Deep Learning Frameworks

Below are the critical observations of how typical distributed machine learning systems consume IO resources. Quiver is designed to take advantage of these.

* Shareability: Data samples are shared across multiple epochs in a single job and across jobs that use the same dataset. This property ensures that caching data is likely to be an improvement.
* Random Access: In each epoch, a dataset is accessed in the order of a random permutation. Thus it is very likely that a cache miss happens if the dataset can not fit in the SSD.
* Substitutability: However, we can cheat the trainer by **giving whatever is in the cache** to the framework, as long as randomness is enforced and each data sample is iterated *exactly once* in an epoch. 
* Predictability: If the model is configured, GPUs are good, then the time needed for each mini-batch is typically constant. We can use this property to estimate what improvement our caching strategy can reach.

![struct]({{ site.url }}{{ site.baseurl }}/assets/images/quiver.png){: .align-center}

## Substitutable Hits

Remember that we can feed whatever is in the cache. Quiver achieves minimal cache miss by doing substitutable hits. In particular, the dataloader will lookup many more indices than a mini-batch it needs(say 10x). This usually makes sure that the cache system has enough data samples to return. After that, those indices not appeared in the cache will be marked as *pending* and requested later by the dataloader to replace what is currently in the local storage.

## Co-ordinated Eviction

To refresh the cache so that each trainer gets its random mini-batches, Quiver needs a strategy to flush the cache. Two properties to satisfy:

* Randomness: mini-batches are uniformly scattered across the dataset
* Alignment: data samples in a mini-batch are best to have similar lengths

Therefore, Quiver segments the dataset into partitions, say 10% of the dataset. A *partition* is further segmented into 10 *stripe* units, so that a logical chunk is the concatenation of stripe units from each **different** partition. In this way, we get the alignment property inside each stripe and randomness across partitions.

Quiver sets a two-step queue for eviction: when another chunk is ready, new requests will go to it. The old one is still available to trainers that are using it, but new fetches are not coming.

## References

Kumar, Abhishek Vijaya, and Muthian Sivathanu. "Quiver: An informed storage cache for Deep Learning." 18th {USENIX} Conference on File and Storage Technologies ({FAST} 20). 2020.