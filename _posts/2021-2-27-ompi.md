---
title: "HPC: Open MPI"
categories:
  - System
  - HPC
---

Open MPI is one of the most popular implementations of the MPI standard. The Open MPI presents code structure with clarity and provides detailed documentation.

For how to write basic multi-processing programs, see [this Github repo](https://github.com/mpitutorial/mpitutorial).

Most descriptions come from the Open MPI document and [FAQ sections](https://www-lb.open-mpi.org/faq/).

## Structure of Open MPI

Open MPI is implemented by the C language, with mainly three components: 

* OMPI: Implmentation of the MPI logic.
* ORTE: The Open Run-Time Environment.
* OPAL: The Open Portable Access Layer. This is the "glue" between the two parts above.

ORTE depends on OPAL, OMPI depends on both ORTE and OPAL. Each component is compiled down to a library and linked by the mpicc wrapper. For instance, if you compile a C MPI code like:

```
mpicc -o main -g main.c
```

It calls the underlying compiler and does the link by default:

```
cc -o main -g main.c -lmpi -lopen-rte -lopen-pal ...
```

## Running Jobs with Scheduler

Open MPI supports multi-node execution natively. For example, if Open MPI is configured and built with PMIx support, it should be easy to both launch MPI applications through the srun command of Slurm and mpirun command from Open MPI. No hostfile and rank size is needed, which is great!

See this [document](https://www-lb.open-mpi.org/faq/?category=slurm) for more details. 

## Fine-grained Control over Open MPI

MCA is another key concept in Open MPI. It means Modular Component Architecture. You can view the whole OMPI project as built with a framework(That's pretty true for the source code). Each component supports part of the functionality inside the project, in different ways. Therefore you are allowed to switch between distinct implementations, try out some parameters to fine-tune the performance of a particular program while keeping its behavior constant.

One critical framework is BTL, the Byte Transfer Layer. It controls how Open MPI does point-to-point communications between processes. By default, Open MPI uses vader BTL for communications within a node, which takes advantage of shared memory access. As for inter-node exchanges, Open MPI can either goes through TCP channels(should always usable), or through high-speed InfiniBand(choose openib or ucx for BTL) if you are running on an HPC cluster.

For example, using Open MPI on Tianhe-2K cluster will usually invoke this warning: 
```
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           cpn210
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
```

This probably indicates that openib is not available on this cluster. Notice that Openib is deprecated in newer versions and replaced by [open ucx](https://www.openucx.org/). Run with BTL specified('^' to exclude a component) to suppress this message:

```
mpirun -np 4 --mca btl ^openib a.out
```

[This FAQ section](https://www-lb.open-mpi.org/faq/?category=openfabrics#ib-components) provides some advice on how to adapt suitable parameters given characteristics of your program. Also, if there is a problem with your program running, check it first.