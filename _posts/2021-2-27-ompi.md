---
title: "HPC: Open MPI"
categories:
  - System
  - HPC
---

Open MPI is one of the most popular implementation of the MPI standard. The Open MPI presents code structure with clarity and provides detailed documentation.

For how to write basic multi-processing programs, see [this Github repo](https://github.com/mpitutorial/mpitutorial).

Most descriptions come from the Open MPI document and [FAQ sections](https://www-lb.open-mpi.org/faq/).

## Structure of Open MPI

Open MPI is implemented by the C language, with mainly three components: 

* OMPI: Implmentation of the MPI logic.
* ORTE: The Open Run-Time Environment.
* OPAL: The Open Portable Access Layer. This is the "glue" between the two parts above.

ORTE depends on OPAL, OMPI depends on both ORTE and OPAL. Each component is compiled down to a library and linked by the mpicc wrapper. For instance, if you compile a C MPI code like:

```
mpicc -o main -g main.c
```

It actually calles the underlying compiler and does the link by default:

```
cc -o main -g main.c -lmpi -lopen-rte -lopen-pal ...
```

## Running Jobs with Scheduler.

Open MPI supports multi-node execution natively. For example, if Open MPI is configured and built with PMIx support, it should be easy to both launch MPI applications through the srun command of Slurm and mpirun command from Open MPI. No hostfile and rank size is needed, which is great!

See this [document](https://www-lb.open-mpi.org/faq/?category=slurm) for more details. 

## Find-grained Control of Open MPI

MCA is another key concept in Open MPI. It means Modular Component Architecture. You can view the whole OMPI project as built with a framework(That's pretty true for the source code). Each component supports part of the functionality inside the project, in different ways. Therefore you are allowed to switch between distinct implementations, try out some parameters to fine-tune the performance for particular program, while keeping its behavior constant.

One critical framework is BTL, the Byte Transfer Layer. It controls how Open MPI does point-to-point communications between processes. Be default, Open MPI uses vader BTL for communications within a node, which takes advantage of shared-memory access. As for inter-node exchanges, Open MPI can either goes through TCP channels(should always usable), or through high-speed InfiniBand(choose openib or ucx for BTL) if you are running on a HPC cluster.

This [FAQ section](https://www-lb.open-mpi.org/faq/?category=openfabrics#ib-components) provides some advice on how to adapt suitable parameters given characteristics of your program. Also, if there is problem on your program running, check FAQ first.