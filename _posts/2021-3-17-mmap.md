---
title: "System: mmap, Memory Mapped I/O"
categories:
  - System
---

Memory-mapped IO through Linux **mmap** command is fundamental to many critical techniques in systems programming. For instance, MPI could use mmap to implement fast communication between processes *within a node*. In-memory file systems require mmap and, optimizations are studied to improve I/O with the secondary storage. This blog covers several naive concepts on how mmap works.

## mmap

The **mmap** command of Linux establishes a mapping between a virtual memory area and a segment of physical memory. The mapping can set as backed by a file stored in a block device or initialized as empty. In the first situation, we conduct operations to a file just like a string located in memory. Mutations are flushed back to disk in an *asynchronous* manner. When no backed file exists, the mapped physical memory can be utilized as *shared memory* for data exchange between processes.

When a process calls the mmap command, it creates a *virtual memory area* to allocates the virtual space. At this point, **no physical memory** is reserved for this mapping, so the mapping itself is empty. That is to say, the process is only **granted** to see that virtual space and the actual mapping is built *lazily* through the Linux **page fault** mechanism.

When you access the backed file through mapping, the process first references its page table. It searches the existence of the mapping between the virtual address and physical memory. Note that initially, no mapping is there, so a *page fault* exception is triggered. Now the kernel takes control, fetches the missing page from the storage, and places it in memory. Finally, the mapping sits in the page table. Swaps may happen.

![mmap]({{ site.url }}{{ site.baseurl }}/assets/images/mmap.png){: .align-center}

And why mmap provides quick I/O(especially read)? Because of the **page cache** in the Linux *virtual file system*. Since I/O through the secondary storage is terribly slow, The VFS maintains(in the kernel space) a page cache to cache data blocks that are repeatedly requested. In a cache miss, the kernel first moves the page to the cache, then to the userspace. However, in a memory *page fault* situation, that data block is copied directly, saving one copy in the kernel space.

## In-memory File Systems

However, page cache doesn't seem to be useless: it is a cache, though. MMIO slows down if page faults happen frequently. Workaround seeks to pre-fetch pages into memory(e.g., several pages that follow). For an in-memory file system, it is trivial to load all pages before doing any file operation. But the cost of **initializing page table** is still there, plus it is common for applications to operate only a minor part of a file. To summarize, we need a guided, dynamic page loading strategy.

In this work, Jungsik Choi et al. propose three techniques to reduce the overhead of performing page mapping:

* Map-ahead. The kernel holds a dynamic loading window: it doubles when the access pattern seems 'sequential' and shrinks to handle random access. Plus, *asynchronous* call is adapted. The kernel returns immediately when the missing page is available while loading continues in the background.
* Mapping cache. When a file is closed, its page entries stay in memory for fast reopening. A hash table and LRU queue are used for quick lookup and dynamic eviction if memory is limited.
* Extended madvise. Now the kernel can respond differently to hints coming from applications. Guided pre-fetch is supported.

## References

Choi, Jungsik, Jiwon Kim, and Hwansoo Han. "Efficient memory mapped file I/O for in-memory file systems." 9th {USENIX} Workshop on Hot Topics in Storage and File Systems (HotStorage 17). 2017.