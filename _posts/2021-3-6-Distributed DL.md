---
title: "AI, System: Distributed Machine Learning"
categories:
  - AI
  - System
---

Spent few days getting into some classic design of parallel, distributed machine learning systems.

[Fedetated learning](/_posts/2020-12-28-securityProjects.md) is quite a different thing. Researchers focus on secure protocols, computation and datasets. But distributed learning is about high performance, easy development and deployment.

## MPI: Simple Collective Operations

Teo, Choon Hui, et al. "A scalable modular convex solver for regularized risk minimization." Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining. 2007.

Agarwal, Alekh, et al. "A reliable effective terascale linear learning system." The Journal of Machine Learning Research 15.1 (2014): 1111-1133.

## Mapreduce: Efficient Abstraction, Fault-tolerance

Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters." Communications of the ACM 51.1 (2008): 107-113.

Chu, Cheng, et al. "Map-reduce for machine learning on multicore." Advances in neural information processing systems 19 (2007): 281.

Ranger, Colby, et al. "Evaluating mapreduce for multi-core and multiprocessor systems." 2007 IEEE 13th International Symposium on High Performance Computer Architecture. Ieee, 2007.

## Spark: In-memory, Iterative, Functional Programming

Zaharia, Matei, et al. "Spark: Cluster computing with working sets." HotCloud 10.10-10 (2010): 95.

Vavilapalli, Vinod Kumar, et al. "Apache hadoop yarn: Yet another resource negotiator." Proceedings of the 4th annual Symposium on Cloud Computing. 2013.