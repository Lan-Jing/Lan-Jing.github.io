---
title: "Parallel Computing: Optimized CUDA Entropy Computation"
categories:
  - Parallel Computing
---

Homework for DCS316 Multi-core Programming.

## Problem Description

Compute the entropy for each point of a 2D matrix, using a 5x5 window. Matrix elements are integer within the range [0, 16). The image below shows the computation with 3x3 windows. 

![entropy]({{ site.url }}{{ site.baseurl }}/assets/images/entropy.png){: .align-center}

## Platform

The Tianhe-2K supercomputer, each GPU node with two Intel Xeon Gold 6132 and 4 NVLINKed Tesla V100 accelerators. This homework only uses one GPU.

## Baseline

The baseline simply maps each CUDA thread to one data point in the matrix and computes its entropy. It first counts the frequency for all appeared elements, then gets the entropy using the formula.

```cpp
__global__ 
void entropy_kernel1(float* d_entropy, char* d_val, 
                     int m, int n)
{
    const int x = threadIdx.x + blockIdx.x * blockDim.x,
              y = threadIdx.y + blockIdx.y * blockDim.y;

    int count[16], eleCount = 0;
    memset(count, 0, sizeof(int)*16);
    for(int dx = -2;dx <= 2;dx++) {
        for(int dy = -2;dy <= 2;dy++) {
            int xx = x + dx,
                yy = y + dy;
            
            if(xx >= 0 && yy >= 0 && yy < m && xx < n) {
                count[d_val[yy*n+xx]]++;
                eleCount++;
            }
        }
    }
    eleCount = max(1, eleCount);

    float entropy = 0;
    for(int k = 0;k <= 15;k++) {
        float p = (float)count[k]/eleCount;
        entropy -= p*log2(p);
    }
    if(y < m && x < n) {
        d_entropy[y*n+x] = entropy;
    }
}
```

Tests are performed 10 times to get the best result. It takes 171ms to complete for an 8192x8192 matrix.

## Store log2(x) In A Lookup Table

Each integer may not appear more than 25 times in a 5x5 matrix. Therefore we can replace these log2(x) function calls with a table lookup:

$$
p\log(p) = p*(logTable[cnt[i]]-logTable[eleCount])
$$

Where cnt[i] is the time it appears and eleCount holds the number of valid elements in the window (that fall in the matrix).

```cpp
for(int k = 0;k <= 15;k++) {
        float p = (float)count[k]/eleCount;
        entropy -= p*(d_logTable[count[k]]-d_logTable[eleCount]);
}
```

Not work, now 184ms for computation. However, we may put the table elsewhere, or more clearly, access it from a different path. You see, since the matrix is random, it is unlikely that one element appears more than 10 even 20 times in a 5x5 window. That means $locality$: elements closed to 0(maybe 1-5?) have higher access frequency. 

So I use Texture Cache to read it. Simply add __ldg(&x) to read the data from Texture Cache instead of L1 Cache:

$$
  \text{Global Memory} \to \text{L2 Cache} \to \text{Tex Cache}
$$

```cpp
for(int k = 0;k <= 15;k++) {
        float p = (float)count[k]/eleCount;
        entropy -= p*(__ldg(&d_logTable[count[k]])-__ldg(&d_logTable[eleCount]));
}
```

The result is great: we have doubled performance, only 97ms is needed.

## Lower Precision

Since integers appear no more than 25 times in a window, there is no need to use int array for count[]. It is sufficient to use char(int8) as data type:

```cpp
    char count[16], eleCount = 0;
    memset(count, 0, sizeof(char)*16);
```

This works because the GPU can output 4x arithmetic operations for int8 data than int. Even better, a smaller data type takes less space in the cache, thus improving the hit rate. This leads to an about 10x speedup, computation time is now 19ms compared to 184ms from the above kernel. 

## Put the Input Matrix into Shared Memory

This is trivial as most elements are read 25 times in a coalesced pattern. Note that each block now has a Halo Region which threads in it only read in matrix elements but don't perform any computation.

```cpp
__shared__ int sd_val[32][32];
const int x = threadIdx.x-2 + blockIdx.x * bsize,
          y = threadIdx.y-2 + blockIdx.y * bsize;
if(x >= 0 && y >= 0 && y < m && x < n) {
    sd_val[threadIdx.y][threadIdx.x] = d_val[y*n+x];
} else {
    sd_val[threadIdx.y][threadIdx.x] = 16;
}
__syncthreads();

char count[17], eleCount = 0;
memset(count, 0, sizeof(char)*17);

if(threadIdx.x >= 2 && threadIdx.x <= 29 && threadIdx.y >= 2 &threadIdx.y <= 29) {
    for(int dx = -2;dx <= 2;dx++) {
        for(int dy = -2;dy <= 2;dy++) {
            char nowVal = sd_val[threadIdx.y+dy][threadIdx.+dx];
            count[nowVal]++;
            eleCount += min(1, 16-nowVal);
        }
    }
}
```

This attempt doesn't lead to a speedup: now 25.4ms. Possible reasons: threads in the Halo Region take equal resources but don't contribute to the computation. These threads introduce more divergence in execution. Read from global memory is coalesced thus cache-friendly, therefore shared memory may not be helpful.

## Put count[] Array into Registers

