---
title: "Distributed System: The Google File System"
categories:
  - System
---

The Google File System, implemented by Sanjay Ghemawat et al., is the foundation of many of Google's large-scale, data-driven applications. The Hadoop File System is a later-implemented open-source version of GFS, while with minor differences. 

Concise introduction can be found in [this HDFS document](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html).

## Basic Assumptions, Design Goals

When I was chatting with Prof. Dan Huang, he said finding the need and goal of a system is more critical than the design itself. We can prove that just from GFS: with some basic assumptions and targets, the design is much simplified and optimized for Google's applications.

### Large-scale, Commodity Machines

The Google File System is designed for massively parallel, distributed applications running on hundreds even thousands of commodity machines. Unlike HPC clusters, machines are unreliable. Failure is common, meaning fault tolerance is essential. Network is slow. Thus the system should be highly distributed to fully utilize network bandwidth.

### Large Files, Regular Access

Applications GFS is to support, consume a huge amount of data(to Terabytes), but regularly. More precisely, most data mutations are appends to the end of files, rather than random inserts. Furthermore, they often present multiple-read-single-write(merging) patterns. It is common in, for instance, distributed machine learning and large-scale data analysis. Therefore, contrary to previous file systems, Google put overall throughput in front of access latency. This preference leads to 64MB(even 128MB) chunk size, fundamental in designing GFS.

## Overview

### Large Chunk Size

A larger chunk size helps a lot to the system performance:

* A large chunk size reduces fragmentation of files, thus reducing the workload on the master to manage the namespace.
* Larger chunk aggregates random, small transports of data, utilizing network more efficiently.
* Clients can hold and modify more data locally, reducing TCP connections with servers from less uploading requests. 

### Single Master

The single master help simplify cooperation between clients and chunkservers. Master is the only server having a global view of the whole cluster and file distribution. A total order of transactions and consistency can be easily achieved, at the cost of introducing a single point of failure. Therefore there are shadow servers as backup. 

Separate data and control flow is a common practice for high performance(same for Lustre). Clients ask the master for locations of file chunks, but data exchange is established directly with chunckservers. Also, the master is responsible for managing data, including balancing data distribution, reclaiming space periodically from deleted or stale chunks, and providing serialized plans of concurrent mutations.

## Consistency guarantees

* Consistent: all clients see the same data, from arbitary replicas.
* Defined: consistent and clients see what is changed by mutations.

As mentioned before, a total order of writes, arranged by the master, is applied to all replicas. A server is selected(temporally) as primary by the master to follow the instructions first then broadcast them to others. Besides, each chunk contains a version number, indicating whether it is up-to-date. A stale replica should exclude itself from the master through the regular chunk report. 

## Fault Torlerance

GFS implements replication. By default, each chunk has three copies spread in three different servers. It's worth mention that GFS chooses to spread servers further across racks. Since servers in a rack still share resources: power, network, etc. In each time interval, the master should gather chunk reports from chunkservers. It will create replicas for chunks not replicated enough. Plus, the master will migrate chunks between servers to enforce load balancing and maximize availability. Notice that chunk reports are bypassed with heartbeat messages from servers to master, and migrations also run periodically. The lease sync between the master and servers relaxes the workload of both.

For metadata, the master adapts usual logging and checkpoints. A checkpoint is represented as B-tree to hold the namespace in its entirety. Operations after a checkpoint are made persistent through writing into the operation logs. They merge into the checkpoint after a while. Therefore, fast recovery of the master should restore the checkpoint then redo mutations currently in the log.

## Mapreduce

![Mapreduce]({{ site.url }}{{ site.baseurl }}/assets/images/mapreduce.png){: .align-center}

Mapreduce is a programming model that enables easy development and deployment of big-data applications. It requires programs to express as a **pipeline** consisting of two operators: *map* and *reduce*. The *map* phase is the main stage, transferring key/value pairs from one domain to another. For example, article/content pairs are mapped to word/count pairs. The second *reduce* step often does simpler jobs: counting, combining and summarizing data (e.g., counting the appearance of specific words) from the previous step.

Mapreduce introduces re-execution to provide high availability and efficiency. To have fault tolerance, the master should reassign failed jobs to other idle workers, keeping the program in progress. Commodity machines may run slow for random reasons. Duplicate jobs can significantly prevent these stragglers from slowing the whole process.

Mapreduce benefits from GFS, not only fault tolerance but also locality. Specifically, A Mapreduce scheduler can assign computations near to the computed data, thus reducing consumption of IO resources. 

## References

Ghemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. "The Google file system." Proceedings of the nineteenth ACM symposium on Operating systems principles. 2003.

Shvachko, Konstantin, et al. "The hadoop distributed file system." 2010 IEEE 26th symposium on mass storage systems and technologies (MSST). Ieee, 2010.

Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters." Communications of the ACM 51.1 (2008): 107-113.